---
layout: ../../layouts/PostLayout.astro
title: "The Entropy Function"
date: 2025-09-27
math: true
bokeh: true
toc: true
bskyPostCid:
series:
  name: "Entropy"
  part: 2

---
import BokehPlot from "../../components/diagrams/BokehPlot";
import EntropyBar from "../../components/diagrams/entropy/EntropyBar";
import EntropyReorder from "../../components/diagrams/entropy/EntropyReorder";
import EntropyGraph from "../../components/diagrams/entropy/EntropyGraph";


#### Table of Contents


<br/>

# 1. Interpreting Shannon

Here again is the "Shannon entropy" of a distribution $p_i$ over a set $S$ with $N$ elements:

$$
H[p_i] = \sum_i p_i \log \frac{1}{p_i}
$$

Its argument is a probability distribution, such $H[p_i]$, $H[p(x)]$, $H[\mathcal{N}(\mu, \sigma^2)]$, or a random variable $H[X]$ with its distribution given elsewhere. The square brackets $H[\:\cdot\:]$ should be read as indicating that it is not really a "function" of its argument's numeric value; instead it is a function of the entire distribution. 

The simplest example is the entropy of a uniform distribution over $N$ elements:

$$
\begin{align}
H[\text{uniform}(N)] &= H\left[\left\{\frac{1}{N}, \frac{1}{N}, \ldots, \frac{1}{N} \right\}\right] \\
  &= \sum_{i =0}^N \frac{1}{N} \log{N} \\
  &= \log{N}
\end{align}
$$

We get $\log{N}$, which is easily seen as the "number of bits required to count or label a set of $N$ elements" (in whatever logarithm base we're using)

<EntropyBar total={16} weights={[4,4,4,4]} client:visible />

and the formula also interpolates cleanly over non-uniform distributions

<EntropyBar total={16} weights={[8, 5, 3]} client:visible />


We saw in the first post in this series that it is interpreted as representing the "information density" or "innate uncertainty" of a sample from this distribution. 

It is also the "average log-likelihood per sample" for a typical sequence drawn from the distribution $p_i$:

$$
H[p_i] = \lim_{K\to\infty} \frac{1}{K} \log {\text{Pr} (x_1, \ldots, x_K \mid \text{each } i \text{ occurs } p_i K \text{ times.})}
$$

We also saw that the same formula arises when taking the large-$K$ limit of a log of a multinomial
$$
H\left[\frac{k_1}{K}, \frac{k_2}{K}, \ldots\right] = \lim_{K \to \infty} \frac{1}{K} \log {K \choose k_1, k_2, \ldots}
$$

which can be seen as the relative size of a given outcome of many samples of a uniform distribution.

We can arrive at another characterization by noting that the entropy can be written as an expectation:

$$
H[ p_i ] = \mathrm{E}\left[\log{\frac{1}{p_i}}\right]
$$

where the object inside the expectation is either the "the number of bits required to express that probability" or the "information required to enumerate $\frac{1}{p_i}$ elements". For each probability in the uniform distribution $p_i = \frac{1}{N}$ it is $\log{N}$. This is called the "information function" or just "information", written

$$
I(p_i) = \log{\frac{1}{p_i}}
$$

We can then express $H$ as

$$
H[ p_i ] = \mathrm{E}[I(p_i)]
$$

which suggests we interpret the entropy either "average information required to specify an element in $p_i$" or as the "expected information gained by learning the exact value of a a sample from $p_i$".

# 2. Basic Properties

If we start with a single set

<EntropyBar total={18} weights={[18]} client:visible />

and then we divide it in two

<EntropyBar total={18} weights={[9,9,]} client:visible />

the entropy, as measured in base-2 bits, goes up by $1$.

The same applies for any division into two:

<EntropyBar total={18} weights={[6,6,6]} client:visible />
<EntropyBar total={18} weights={[3,3,3,3,3,3]} client:visible />

For each block of weight $n_i$ and fraction $p_i = \frac{n_i}{N}$ of the whole, dividing it into two half-cells increases its information $I(p_i)$ by $1$ bit:

$$
\begin{align}
I\left(\frac{p_i}{2}\right) &= \log{\frac{1}{p_i/2}} \\
  &= \log\frac{1}{p_i} + \log 2 \\
  &= I(p_i) + 1
\end{align}
$$

Therefore, dividing every block in two at once increases the total entropy by $1$ bit:

$$
\begin{align}
H\left[\frac{p_1}{2}, \frac{p_2}{2}, \ldots \right] &= 2 \times \frac{p_1}{2} \log_2{\frac{1}{p_1/2}} + 2 \times \frac{p_2}{2} \log_2{\frac{1}{p_2/2}} + \ldots \\
  &= p_1(\log_2\frac{1}{p_i} + 1) + p_2(\log_2\frac{1}{p_i} + 1) + \ldots\\
  &= H[p_1, p_2, \ldots] + (p_1 + p_2 + \ldots) \times 1 \\
  &= H[p_1, p_2, \ldots] + 1
\end{align}
$$

If we interpret entropy as giving the information required to "address" or "label" the elements of the set, then dividing each element in two can be interpreted as saying: the new distribution $\frac{p_1}{2}, \frac{p_1}{2}, \frac{p_2}{2}, \frac{p_2}{2}, \ldots$ can be addressed by first specifying an element with of the original distribution $p_1, p_2, \ldots$, followed by a single additional bit, 0 or 1, representing the "left" or "right" element of each new subdivision.

Likewise subdividing or "fine-graining" by another factor $f$ adds $\log{f}$ bits:

$$
H\left[\frac{p_1}{f}, \frac{p_2}{f}, \ldots \right] = H[p_1, p_2, \ldots] + \log{f}
$$

Such a "fine-graining" is equivalent to replacing the distribution $p_i$ by its product with a uniform distribution :

$$
H\left[\frac{p_1}{f}, \frac{p_2}{f}, \ldots \right] = H\left[ p_i \times \{ \frac{1}{f}, \frac{1}{f}, \ldots \} \ldots \right]
$$

which leads us to a general rule that entropies *add* over products:

$$
\begin{align}
H[p_i \times q_j] &= \sum_i \sum_j p_i q_j \log \frac{1}{p_i q_j} \\
  &= \sum_j q_j \sum_i \log \frac{1}{p_i} + \sum_i p_i \sum_j q_j \log \frac{1}{q_j} \\
  &= \sum_i \log \frac{1}{p_i}  + \sum_j q_j \log \frac{1}{q_j} \\
  &= H[p_i] + H[q_j]
\end{align}
$$

Note this is the same as the behavior of a logarithm on numbers:

$$
\log{xy} = \log{x} + \log{y}
$$

and the two laws coincide in the case of uniform distributions[^log]:

$$
\begin{align}
H\left[\text{uniform}(N) \times \text{uniform}(M)\right] &= H\left[\text{uniform}(N)\right] + H\left[\text{uniform}(M)\right] \\
  &= \log N + \log M \\ 
  &= \log NM \\
  &= H\left[\text{uniform}(NM)\right]
\end{align}
$$

[^log]: I find this law to be very suggestive—entropy appears to act like a generalization of "logarithms" to the large space of distributions, where we identify the uniform distributions with the original space of numbers themselves. We can follow the analogy in the other direction to interpret logarithms as an "information" function in all cases. Aren't integers, after all, just an abstract "count" of something?

We can go in the other direction to calculate the entropy of a coarse-graining or aggregating operation. For any distribution $p_i$ there will be some common denominator with which we can write probability as a fraction: $\frac{n_i}{N}$. For example if our probabilities are $0.9, 0.099, 0.001$ we can write these as $\frac{900}{1000}, \frac{99}{1000}, \frac{1}{1000}$. Then we can imagine constructing this distribution by first subdividing into $N$ blocks, then merging blocks to wind up with our final distribution. Merging $n$ blocks will change the entropy from

$$
\begin{align}
H\left[ \frac{n}{N}, \ldots \right] &= \frac{n}{N} \log{ \frac{1}{n/N} } + \ldots\\
  &= \frac{n}{N} (\log{N} + \log{\frac{1}{n}}) + \ldots \\
  &= \bigg(\underbrace{\frac{1}{N} + \frac{1}{N} + \ldots}_{n \text{ times}}\bigg) \log{\frac{1}{1/N}} - \frac{n}{N} \log n + \ldots \\
  &= H\big[ \underbrace{\frac{1}{N}, \frac{1}{N}, \ldots}_{n \text{ times}}\big] - \frac{n}{N} \log n
\end{align}
$$

So grouping $n$ cells into one of size $\frac{n}{N}$ decreases the entropy by a fraction $\frac{n}{N}$ of $\log{n}$, here by $\frac{4}{12}$ of $\log{4}$ which is $0.66\bar{6}$:

<EntropyBar total={12} weights={[]} client:visible />
<EntropyBar total={12} weights={[4]} client:visible />

And grouping all $N$ cells into one of course brings the entropy down to zero:
<EntropyBar total={12} weights={[]} client:visible />
<EntropyBar total={12} weights={[12]} client:visible />


Note that the term subtracting in the above is exactly the (fraction occupied by the merged block) $\times$ (the entropy of a uniform distribution with all elements the size of the merged block). We can create any distribution of blocks by merging a uniform into groups $n_i$:

$$
\begin{align}
H\left[ \frac{n_i}{N}, \ldots \right] &= H\bigg[ \underbrace{\frac{1}{N}, \frac{1}{N}}_{n \text{ times}}, \ldots\bigg] - \sum_i \frac{n_i}{N} \log n_i \\
 &= H[\text{uniform}(N)] - \sum_i \frac{n_i}{N} H[\text{uniform}(n_i)]
\end{align}
$$

Rearranging gives

$$
H\left[ \frac{n_i}{N} \right] +  \sum_i \frac{n_i}{N} H[\text{uniform}(n_i)] = H[\text{uniform}(N)]
$$

in which we can read two the entropy of two equivalent algorithms:
1. either we first select a block from an uneven distribution $\{ \frac{n_i}{N} \}$, then select uniformly from within that block according to a $\text{uniform}(n_i)$
2. or, we select one element from a $\text{uniform}(N)$ distribution

Clearly we should be able to count or address a set of $N$ elements either way—these two selection processes are the same; they must contain or require the same amount of information, hence, their entropies must also be the same.[^derivingH]

[^derivingH]: One can also derive the formula for $H$ in the first place by asserting a handful of properties like continuity, the logarithm-like property on product distributions, and the equivalence of the two selection processes just given; see [wiki](https://en.wikipedia.org/wiki/Entropy_(information_theory)#Alternative_characterization).

# 3. Visualizing Shannon

Each term in the Shannon entropy has the form

$$
p \log{\frac{1}{p}}
$$

whose graph looks like

<br/>
<BokehPlot htmlPath="/data/entropy/pi_plot.html" client:visible />
<br/>

That's an odd shape, but it doesn't mean much on its own. It is the product of $p$ (which increases linearly) and $\log{1/p}$ which goes to $\infty$ at $p \to 0$ and goes to $0$ at $p \to 1$.  

More informative is the graph of $H$ on a two-element distribution, such as that of an unbalanced coin that comes up heads with probability $p$. The distribution is simply $\{p, 1-p\}$ and the entropy, as a function of $p$, looks like the sum of the graph above with itself reflected horizontally, which smooths out the asymmetry giving


<br/>
<BokehPlot htmlPath="/data/entropy/pH_plot.html" client:visible />
<br/>

It's nearly a perfect circle, peaking at a value of $H = \log{2}$ at $p = 0.5$. (In base-2 its peak value would be $1$.)

We can also visualize the entropy over all possible probability distributions on a three-element set. The three probabilities $p_1, p_2, p_3$ are constrained to the two-dimensional simplex $p_3 = 1 - p_1 - p_2$. The entropy function looks like:

<br/>
<BokehPlot htmlPath="/data/entropy/p3_plot.html" client:visible />
<br/>

Again we see that the entropy $H$ attains its highest value in the center (a uniform distribution) and goes to zero in all three corners (the indicators on any of the three elements). It treats each probability symmetrically; it is indifferent to the ordering of the three.

For more than three probabilities no direct visualization is possible. Instead, let us try to say something about the shape of the space and the range of entropies on it.

Within the space of all possible distributions $p_i$ on an $N$-element set, there are $N$ indicators $\mathbf{1}_i = \{\ldots, p_{i-1} = 0, p_i = 1, p_{i+1} = 0, \ldots\}$, each with entropy $H=0$, and one $\text{uniform}(N) = \{ \frac{1}{N}, \frac{1}{N}, \ldots\}$ with entropy $H = \log(N)$. All other distributions $\{p_i\} fall somewhere between these two extremes:

![](../../images/entropy/indic_to_uni.png)

What happens in the middle? What fraction of all possible distributions have the given entropies? Certainly there are vastly more uneven distributions than either the indicators or the single uniform.

Let's first try to take on a simpler problem by considering only distributions which can arise by grouping $N$ underlying cells. This is then the problem of "counting partitions of a set". However, there are no longer $N$ distinct indicators but only one—the partition consisting of the whole set. Clearly there will be more distributions in between, but what is the exact shape?

![](../../images/entropy/one_indic.png)

Let's play with a few, starting from the single partition $\mathbf{1}$ with $H[\mathbf{1}] = 0$. Chipping away at it increases the entropy...

<EntropyBar total={16} weights={[16]} client:visible />
<EntropyBar total={16} weights={[15, 1]} client:visible />
<EntropyBar total={16} weights={[14, 2]} client:visible />
<EntropyBar total={16} weights={[14, 1, 1]} client:visible />
<EntropyBar total={16} weights={[12, 4]} client:visible />
<EntropyBar total={16} weights={[12, 2, 2]} client:visible />
<EntropyBar total={16} weights={[8, 8]} client:visible />

... but the three-way partitions quickly overtake the two-way ones, and of course there are a *lot* of these. The total number of partitions of a set of size $N$ is given by the $N$th [Bell number](https://oeis.org/A000110); the 16th Bell number is around 10 billion. Many of these have the same entropies, though; so the problem of determining the distribution of entropies is actually quite tractable. We only need to calculate the entropies of distinct integer partitions (e.g. $\{14, 1, 1\}$), and the number of these is much smaller; the 16th is only 231. It's then relatively straightforward to determine how many subdivision of a *set* correspond ot each partition—for the single partition into $\{4, 3, 3, 2, 2, 1, 1\}$, the answer is given by a multinomial times a factor for rearranging amongst the pairs of blocks of size $3,2,1$:

$$
\frac{16!}{4!(3!)^2(2!)^2 (1!)^2} \times \frac{1}{(2!)^3} = 756,756,000
$$

As it's 2025, I can ask an AI to count them all in about two minutes.[^chatgpt] Here's a visualization:

[^chatgpt]: Here's the [chat](https://chatgpt.com/share/68e0b77e-458c-800b-b3a8-1348cdf31808), including a lot of extra—and impressive—analysis.

<br/>
<BokehPlot htmlPath="/data/entropy/entropy_counts16.html" client:visible />
<br/>

There's a clear shape, peaking (the AI tells me) near $\log(16) = 2.77$. Neat. Note the log scale: at the peak value there appear to be about 800 million set-partitions!

Returning to the first question: what if we did the same for the set of distributions on $N$ elements? The question now is to estimate the measure attaining each value of $H$ within the $(N-1)$-simplex formed by the $N$ probabilities. This is probably hard to calculate, but should be fairly simple to estimate numerically. Again I conjure a visualization from the AI:

<br/>
<BokehPlot htmlPath="/data/entropy/entropy_measure16.html" client:visible />
<br/>

Well: it's a similar shape. Note the y-axis is not a log-scale in this one, as the whole region near 0 would go to $-\infty$. Still we see that the vast majority of distributions have entropies a bit lower than the maximum. This isn't too enlightening, but I was curious.

Before we move on, I want to register a couple of stray thoughts:
- I find the distinction between the entropy of partitions and distributions here to be suggestive. I suspect that entropy is more naturally defined on partitions: elements with $p=0$ effectively don't exist from the perspective of the entropy. All indicator distributions are the same thing, information-wise (it's the underlying set which is different). I'll have to think on this.
- The first visualization acquires its shape only from the integer-partitions with the highest set-partition multiplicities. It might make more sense to smooth this curve out, so as actually get a sense of the measures in the neighborhood of each $H$ value.
- I don't think a "uniform measure" makes sense on the space of real-valued probability distributions, so the second visualization just given is probably not very meaningful. It might actually make sense to use the entropy itself as the measure on that space, as $H$ is the asymptotic measure a distribution arising from samples from a uniform, by the third argument from the previous post in this series. But I won't try to take that on now.


# 4. Entropy vs. Variance

Here's one visualization of some entropies. Now—unlike all the earlier examples—we will associate the indices $i$ with their numeric value, i.e. treating this as a random variable (henceforth "r.v.") $X \sim p_i$ with $X(i) = i$. This will let allow us to compute a mean and variance for the same distribution and, in partiular, to compare the entropy to the variance:

<br/>
<EntropyReorder n={20} client:visible />
<br/>

Playing with this, we can observe a few things:
- the uniform distribution has the highest possible entropy for a given number of bins.
- all of the indicator/delta-function distributions have entropy zero (and variance zero).
- in general, entropy will be low for very peaked distributions, and high for spread-out ones. In this respect it is similar to the variance.
- but the entropy is unchanged when the cells are shuffled, while the variance will tend to vary a lot.

(Note that you can put in unnormalized distributions, but they will be normalized before calculating the stats.)

Both the entropy and variance characterize the "uncertainty" or "spread-out-ness" of a distribution. Both are zero for an indicator and large for a uniform distribution—but the entropy attains its highest possible value on a uniform, while it's easy to make the variance even larger by creating something bimodal. (If you click the "Beta" button enough times you'll get something bimodal, or you can draw your own.)

What's the difference?

We'll write the entropy of a r.v. $X$ as a function of the r.v. rather than of the probabilities themselves:

$$
\begin{align}
H[X] = \mathrm{E}\left[\log{\frac{1}{p(X)}}\right] && \mathrm{Var}(X) = \mathrm{E}[X^2] - \mathrm{E}[X]^2
\end{align}
$$

Despite this notation, the entropy does not depend on the *value* of $X$, only on the probabilities, while the variance does depend on the values attained by $X$.

Yet if we plot the two against each other for a few typical distributions, they appear to be closely related:

<br/>

<EntropyGraph dataPath='/data/entropy/entropy_var_examples.json' client:visible />

<br/>

(The normals and betas here are discretized, rather than continuous distributions. We'll address the entropy of continuous distributions another time.)

What's going on?

Let's investigate further. Here are a family of normal distributions centered on $0.5$:

<EntropyGraph dataPath='/data/entropy/entropy_var_normals.json' client:visible />

We see a clear pattern, which interpolates between the nearly-indicator-like $\sigma^2 = 0.0001$ and the nearly-uniform $\sigma^2 = 0.1$.

Here now are a family of $\text{Beta}(\alpha, \beta)$  distributions, representing the posterior probability $p$ that an unfair coin (whose actual $p = 0.25$) comes up heads. At first we have only a broad idea of the value of $p$, but over many flips our posterior belief converges to the true value.

<EntropyGraph dataPath='/data/entropy/entropy_var_betas.json' client:visible />

Again wee see entropies and variances which decreasing together. 

But all of these distributions have been approximately normal; guided by our first widget we probably need to be looking at more spread-out distributions. 

Let's try a contrived scenario: you flip a coin of unknown $H$-probability $p$ repeatedly and get the same result $K$ times in a row, but you don't know if these are $H$ or $T$. The probability of such a streak given $p$ is


$$
\begin{align}
\text{Pr}(K \text{ in a row} \mid p) &= p^K + {(1-p)}^K \\
  &= \text{Beta}(K+1, 1) + \text{Beta}(1, K+1)
\end{align}
$$

If we take our prior on $p$ to be uniform (i.e. a $\text{Beta}(1,1)$), the posterior distribution of $p$ goes as

$$
\begin{align}
\text{Pr}(p \mid K) &\propto \text{Pr}(K \mid p) \times \text{Pr}(p) \\
  &\propto (\text{Beta}(K+1, 1) + \text{Beta}(1, K+1))\times \text{Beta}(1, 1) \\
  &\propto \text{Beta}(K+2, 2) + \text{Beta}(2, K+2)
\end{align}
$$

which looks like

<EntropyGraph dataPath='/data/entropy/entropy_var_betas_bimodal.json' client:visible />

There's still a tight relationship between entropy and variance, but now it goes the other way.

The difference seems to be that these new distributions are bimodal. 

Apparently we can change the variance of a distribution quite freely without affecting the entropy, simply by shuffling it to be more or less spread out. 

Then if find can find some family of distributions with equal variances, then their entropies must vary.

Let's try it. In the following I started with a $\mathcal{N}(0.5, 0.1)$, and then had the AI conjure up a handful of shuffled versions of that distribution (which should have different variances) and a handful of Beta distributions with this same variance but otherwise different shapes (which should therefore have different entropies):


<EntropyGraph dataPath='/data/entropy/entropy_var_shuffled_betas.json' client:visible />

The point is: entropy and variance are closely related *for unimodal distributions*, but not in general. They *can* vary independently, though it can take some pretty contrived distributions to demonstrate it.

Further investigation sheds some light. Apparently, there exists a series expansion for the Shannon entropy in terms of the [cumulants](https://en.wikipedia.org/wiki/Cumulant) of a distribution, which are certain functions $\kappa_n$ of the moments which distribute across independent distributions: $\kappa_n(X_1 + X_2) = \kappa_n(X_1) + \kappa_n(X_2)$.[^z] Up through $\kappa_3$, they are identical with the moments—$\kappa_2$ is just the variance. It seems that any distribution can be expanded in a series in its cumulants around a normal distribution, called an [Edgeworth series](https://en.wikipedia.org/wiki/Edgeworth_series). Hence there exists a "local approximation" of the entropy which will be fairly accurate for distributions which are close to normal:

$$
\begin{align}
H[X] &= H[\mathcal{N}(\mu, \sigma^2)] - \frac{\kappa_3^2}{12 \sigma^2} + \frac{\kappa_4}{24\sigma^4} + \ldots \\
     &= \frac{1}{2}\log{2\pi e \sigma^2} - \ldots
\end{align}
$$

[^z]: In statistical mechanics, the logarithm of the partition function, $\ln Z$, turns out to be a <a href="/posts/2024-09-30-partition-function/#z-is-a-few-kinds-of-generating-functions-at-once">cumulant generating function</a>, which is one way of explaining its many surprising properties.

This was surprising to me, since the Shannon entropy *doesn't* depend on the values taken by the R.V. $X$, while the variance and cumulants do. How can it be that $\sigma^2$ shows up in the formula for $H$, then? The explanation is that, here, we are specifying *which distribution we are talking about* by its variance and cumulants; the entropy depends on these because it depends on the distribution. In other words, the value of the entropy is a function of the parameters of the distribution:

$$
 H[X(\sigma^2, \ldots)] = h(\sigma^2, \ldots)
$$


<br/>


<br/>
<hr/>
<br/>

*Interactive visualizations for this post were authored with [D3.js](https://d3js.org/) in Typescript, embedded in a [Marimo](https://marimo.io/) (Python) notebook via [anywidget](https://anywidget.dev/) with considerable help from Claude Code, and then manually ported into React components to be consumed by Astro, which builds this site. The simpler plots were built with [Bokeh](https://www.bokeh.org/) and exported as HTML. Mostly this was a pain and I wouldn't do it this way again, but it is worth noting that Claude is excellent at one-off D3-type visualizations.*
